{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive PDE discretizations on cartesian grids : Algorithmic tools\n",
    "\n",
    "## Part : Automatic differentiation\n",
    "## Chapter : Reverse automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook illustrates *reverse first order* automatic differentiation. Recall that this approach is recommended for functions from a large dimensional space, to a small dimensional space, and whose jacobian does not have a sparse structure. It is typically appropriate for large scale optimization problems.\n",
    "\n",
    "**Disclaimer.** Reverse first order automatic differentiation is a classical feature, found in many packages better optimized and maintained than this one. If you only need this specific feature, then it could be wise to use another implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "[**Summary**](Summary.ipynb) of this series of notebooks. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "[**Main summary**](../Summary.ipynb), including the other volumes of this work. "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "# Table of contents"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "  * [1. Generating variables and requesting an expression's derivatives](#1.-Generating-variables-and-requesting-an-expression's-derivatives)\n",
       "    * [1.1 Gradient](#1.1-Gradient)\n",
       "    * [1.2 Hessian operator](#1.2-Hessian-operator)\n",
       "  * [2. General operators and their adjoints](#2.-General-operators-and-their-adjoints)\n",
       "    * [2.1 Linear mapping](#2.1-Linear-mapping)\n",
       "    * [2.2 Linear inverse](#2.2-Linear-inverse)\n",
       "    * [2.3 Non-linear operator](#2.3-Non-linear-operator)\n",
       "  * [3 Loops](#3-Loops)"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "\n",
       "**Acknowledgement.** The experiments presented in these notebooks are part of ongoing research, \n",
       "some of it with PhD student Guillaume Bonnet, in co-direction with Frederic Bonnans.\n",
       "\n",
       "Copyright Jean-Marie Mirebeau, University Paris-Sud, CNRS, University Paris-Saclay\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys; sys.path.append(\"..\") # Allow imports from parent directory\n",
    "from Miscellaneous import TocTools; TocTools.displayTOC('Reverse','Algo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Known bugs and incompatibilities.** Our implementation of automatic differentiation is based on subclassing the numpy array class. While simple and powerful, this approach suffers from a few pitfalls, described in the notebook linked below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Notebook [ADBugs](./Notebooks_Algo/ADBugs.ipynb) , from volume Algo [Summary](./Notebooks_Algo/Summary.ipynb) "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TocTools.MakeLink('ADBugs','Algo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse; import scipy.sparse.linalg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NumericalSchemes.AutomaticDifferentiation as ad\n",
    "import NumericalSchemes.FiniteDifferences as fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LInfNorm(a): return np.max(np.abs(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generating variables and requesting an expression's derivatives\n",
    "\n",
    "Reverse automatic differentiation works by keeping a history of the computations. The sensitivity of the result w.r.t some components is obtained by propagating the sensitivities backward in the computation queue and using the operator adjoints.\n",
    "\n",
    "Our implementation of the reverseAD is differs from the denseAD or sparseAD classes: here we do not define an data type overloading the arithmetic operators and the basic special functions. Instead, the reverseAD class is only meant to keep a history of user specified computations.\n",
    "\n",
    "This section only shows how to generate variables, and request an expression's derivatives. See the next section for an actual use of automatic differentiation.\n",
    "For a beginning, we create an empty history.\n",
    "<!---Our implementation of reverse automatic differentiation is probably --->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0=np.linspace(0,np.pi,4)\n",
    "gridScale=x0[1]-x0[0]\n",
    "u0=np.sin(x0)\n",
    "v0=np.cos(x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty history\n",
    "rev = ad.Reverse.empty()\n",
    "# Create AD variables w.r.t which the gradient will be required\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated AD variables are of sparse AD type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u1: spAD(array([0.00000000e+00, 8.66025404e-01, 8.66025404e-01, 1.22464680e-16]), array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[0],\n",
      "       [1],\n",
      "       [2],\n",
      "       [3]]))\n",
      "v1: spAD(array([ 1. ,  0.5, -0.5, -1. ]), array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]]), array([[4],\n",
      "       [5],\n",
      "       [6],\n",
      "       [7]]))\n"
     ]
    }
   ],
   "source": [
    "print(\"u1:\",u1)\n",
    "print(\"v1:\",v1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we make some computation using the variables registered in the reverseAD class, then we can request the gradient of the final expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = (u1**2+v1**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.73205081e+00,  1.73205081e+00,  2.44929360e-16,\n",
       "        2.00000000e+00,  1.00000000e+00, -1.00000000e+00, -2.00000000e+00])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.gradient(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If needed, this expression can be reshaped similar to the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_grad,v_grad = rev.to_inputshapes(rev.gradient(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert LInfNorm(v_grad-2*v0) < 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(1,2)]==[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Hessian operator\n",
    "\n",
    "The hessian operator may be accessed as well, using the Reverse2 submodule. Note that the hessian matrix itself is never assembled, since in applications it would typically be dense and of large size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty history\n",
    "rev = ad.Reverse2.empty()\n",
    "# Create AD variables w.r.t which the gradient will be required\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)\n",
    "result = (u1**2+v1**2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad = rev.gradient(result) # Gradient\n",
    "hess = rev.hessian(result) # Hessian operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.73205081e+00,  1.73205081e+00,  2.44929360e-16,\n",
       "        2.00000000e+00,  1.00000000e+00, -1.00000000e+00, -2.00000000e+00])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this specific example, the hessian operator is twice the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  3.46410162e+00,  3.46410162e+00,  4.89858720e-16,\n",
       "        4.00000000e+00,  2.00000000e+00, -2.00000000e+00, -4.00000000e+00])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hess(grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. General operators and their adjoints\n",
    "\n",
    "We illustrate Reverse automatic differentiation, in its intended use case involving operators operators whose jacobians should, in principle, be both high-dimensional and non-sparse. Note that linear operators often correspond to this desciption. For instance:\n",
    "* A linear mapping, given in sparse form, but iterated many times.\n",
    "* The inverse of a linear mapping, given in sparse form.\n",
    "* The fast fourier transform, etc\n",
    "\n",
    "Obviously, we also address non-linear operators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Linear mapping\n",
    "\n",
    "We first construct some sparse matrix, for the exposition, based on a finite difference scheme.\n",
    "Here we consider a transport scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransportScheme(u,h,speed,dt):\n",
    "    return u+dt*speed*fd.DiffUpwind(u,(1,),h,padding=0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to construct the matrix, we evaluate the scheme on a sparse AD variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "speed = 1.; T=1.5; nsteps=5; dt = T/nsteps;\n",
    "transport_ad = TransportScheme(ad.Sparse.identity(u0.shape),gridScale,speed,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_matrix = scipy.sparse.coo_matrix(transport_ad.triplets()).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = ad.Reverse.empty()\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)\n",
    "\n",
    "u2 = rev.apply_linear_mapping(transport_matrix,u1**2,niter=nsteps)\n",
    "\n",
    "result = (u2*v1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the numerous iterations, the variable $u_2$ does not depend in a sparse manner on $u_1$. However, the dependence is linear, and thus has a simple adjoint, which will be exploited.\n",
    "The variable $u_2$ is of sparseAD type, but features negative placeholder indices. Likewise for the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spAD(array([5.02050076e-01, 4.17158585e-01, 1.38706040e-01, 2.77367654e-33]), array([[1.],\n",
       "       [1.],\n",
       "       [1.],\n",
       "       [1.]]), array([[-1],\n",
       "       [-2],\n",
       "       [-3],\n",
       "       [-4]]))"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spAD(array(0.64127635), array([ 1.00000000e+00,  5.00000000e-01, -5.00000000e-01, -1.00000000e+00,\n",
       "        5.02050076e-01,  4.17158585e-01,  1.38706040e-01,  2.77367654e-33]), array([-1, -2, -3, -4,  4,  5,  6,  7]))"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  8.03222547e-01,  6.77741743e-01, -2.49367755e-17,\n",
       "        5.02050076e-01,  4.17158585e-01,  1.38706040e-01,  2.77367654e-33])"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.gradient(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the validity of the result using dense automatic differentiation, since this specific instance is small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=x0.size\n",
    "#u1 = ad.Dense2.identity(constant=u0)\n",
    "u1 = ad.Dense2.identity(constant=u0,shift=(0,n))\n",
    "v1 = ad.Dense2.identity(constant=v0,shift=(n,0))\n",
    "\n",
    "u2 = ad.apply_linear_mapping(transport_matrix,u1**2,niter=nsteps)\n",
    "#u2 = ad.apply_linear_mapping(transport_matrix,u1**2)\n",
    "#result=u2.sum() #\n",
    "result = (u2*v1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "denseAD(array(0.64127635),\n",
       "array([ 0.00000000e+00,  8.03222547e-01,  6.77741743e-01, -2.49367755e-17,\n",
       "        5.02050076e-01,  4.17158585e-01,  1.38706040e-01,  2.77367654e-33]))"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.dot(result.coef2,result.coef1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same works for second order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad.reload_submodules()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = ad.Reverse2.empty()\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)\n",
    "\n",
    "u2 = rev.apply_linear_mapping(transport_matrix,u1**2,niter=nsteps) # Also inserted quadratic non-linearity\n",
    "#u2 = rev.apply_linear_mapping(transport_matrix,u1**2)\n",
    "#result=u2.sum() #\n",
    "result = (u2*v1).sum()\n",
    "\n",
    "grad = rev.gradient(result)\n",
    "hess = rev.hessian(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  8.03222547e-01,  6.77741743e-01, -2.49367755e-17,\n",
       "        5.02050076e-01,  4.17158585e-01,  1.38706040e-01,  2.77367654e-33])"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2 = hess(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  1.20144921e+00,  1.10232870e+00,  6.28712488e-17,\n",
       "        8.66488999e-01,  6.93122236e-01,  2.17099574e-01, -1.12957547e-33])"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.440892098500626e-16"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LInfNorm(r2-r1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Linear inversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Scheme(u,h):\n",
    "    return fd.DiffUpwind(u,(1,),h,padding=0.)\n",
    "\n",
    "residue = Scheme(ad.Sparse.identity(u0.shape),gridScale)\n",
    "matrix = scipy.sparse.coo_matrix(residue.triplets()).tocsr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def mysolve(u,co_output=None):\n",
    "    solver = scipy.sparse.linalg.spsolve\n",
    "    if co_output is None:\n",
    "        return solver(matrix,u)\n",
    "    else: \n",
    "        return [(solver(matrix.T,co_output),u)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since solving linear problems is a common operation, a factory method `ad.Reverse.linear_solution_with_adjoint` is provided to produce functions equivalent to `mysolve`. It is illustrated in a further subsection.\n",
    "\n",
    "Two equivalent syntaxes are available to evaluate a function using reverse automatic differentiation:\n",
    "* Using `rev.apply`, where rev is the reverseAD variable keeping the history of the computations.\n",
    "* Using `ad.apply`, and specifying the `reverse_history = rev` optional argument. This latter approach has the advantage of being compatible with the `envelope` keyword for differentiating extrema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = ad.Reverse.empty()\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)\n",
    "\n",
    "u2 = rev.apply(mysolve,u1)\n",
    "u3 = ad.apply(mysolve,u2,reverse_history=rev) # Equivalently : u3=rev.apply(mysolve,u2) \n",
    "\n",
    "result = (u3*v1).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both `u2`, `u3` and the result contains *virtual* symbolic perturbations, with negative indices. They are eliminated in the backpropagation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spAD(array(5.69821876), array([ 1.00000000e+00,  5.00000000e-01, -5.00000000e-01, -1.00000000e+00,\n",
       "        4.74851563e+00,  2.84910938e+00,  9.49703126e-01,  1.34297549e-16]), array([-5, -6, -7, -8,  4,  5,  6,  7]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.09662271, 2.74155678, 3.83817949, 3.83817949]),\n",
       " array([4.74851563e+00, 2.84910938e+00, 9.49703126e-01, 1.34297549e-16]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.to_inputshapes(rev.gradient(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this simple case, the gradient may also be computed by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.09662271, 2.74155678, 3.83817949, 3.83817949]),\n",
       " array([4.74851563e+00, 2.84910938e+00, 9.49703126e-01, 1.34297549e-16]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def co_mysolve(co_output): return mysolve(u0,co_output=co_output)[0][0]\n",
    "co_mysolve(co_mysolve(v0)), u3.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Non-linear operator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Loops\n",
    "\n",
    "\n",
    "**Note on complexity** Reverse automatic differentiation requires saving program state at each intermediate computation steps.\n",
    "If a program contains a loop, this may result in severe memory usage. Therefore, it is common to only store a small proportion of the intermediate states, referred to as keypoints, and to use recomputations for reconstructing the other steps. For best efficiency, this procedure must be made recursive. \n",
    "*TODO : We will provide a helper function for that purpose.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysolve = ad.Reverse.linear_inverse_with_adjoint(scipy.sparse.linalg.spsolve,matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rev = ad.Reverse.empty()\n",
    "u1 = rev.identity(constant=u0)\n",
    "v1 = rev.identity(constant=v0)\n",
    "\n",
    "u3 = rev.iterate(mysolve,u1,niter=2)\n",
    "#u3 = rev.apply_linear_inverse(matrix,scipy.sparse.linalg.spsolve,u1,niter=2) # Equivalent\n",
    "\n",
    "result = (u3*v1).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spAD(array(5.69821876), array([ 1.00000000e+00,  5.00000000e-01, -5.00000000e-01, -1.00000000e+00,\n",
       "        4.74851563e+00,  2.84910938e+00,  9.49703126e-01,  1.34297549e-16]), array([-5, -6, -7, -8,  4,  5,  6,  7]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.09662271e+00, 2.74155678e+00, 3.83817949e+00, 3.83817949e+00,\n",
       "       4.74851563e+00, 2.84910938e+00, 9.49703126e-01, 1.34297549e-16])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev.gradient(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
